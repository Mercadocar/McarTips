Kubernetes/Helm/Azure/DevOps Adventure.
A poor man guide to master orchestration.

arquitetura@mercadocarmercantil.onmicrosoft.com
Dovu41293

I just created the k8s on azure. Let´s try to access it locally.
    Gotta install Azure CLI.
    https://docs.microsoft.com/pt-br/cli/azure/install-azure-cli?view=azure-cli-latest
    Run az login
    Set the same cred. used to gotto azure portal.
    "az aks install-cli" to install AKS. Set the variables as it commands.
    Then find the resource group, the k8s cluster name and run:
    (this command below actually works today)
    az aks get-credentials --resource-group Arquitetura --name k8s-arquitetura
    "kubectl get nodes" to see if works
    Nice. Now we can control k8s on azure locally.

Let´s try to deploy our Telepreço POC on k8s.
Then I realised that our docker images are hosted on dockerhub (private ones)
We need to let kubernetes know it and make it able to pull images from there.
So lets go:
    export DOCKER_REGISTRY_SERVER=https://index.docker.io/v1/
    export DOCKER_USER=Type your dockerhub username, same as when you `docker login`
    export DOCKER_EMAIL=Type your dockerhub email, same as when you `docker login`
    export DOCKER_PASSWORD=Type your dockerhub pw, same as when you `docker login`

    kubectl create secret docker-registry myregistrykey \
    --docker-server=$DOCKER_REGISTRY_SERVER \
    --docker-username=$DOCKER_USER \
    --docker-password=$DOCKER_PASSWORD \
    --docker-email=$DOCKER_EMAIL

    Then the image will be like this on yaml:
    image: index.docker.io/rodoflho/mcarbabybfftelepreco:latest
These commands were for private repos on docker hub, but when they are public
just ignore and lets continue.

Now, lets edit the yaml to make BFF work. Gonna list my struggle in order of happening:
    - API versions: v1, v1-beta, I dont know...some of them are not contemplating
    some keywords like deployment (in vase of v1) and NodePort.
    - Type: NodePort should be in same level than spec
    - First I added an deployment, ok. But then tried to add a service too, on the same file.
    The service wanted to be created, but deployment not. I would like something incrementable.
    - Nice! The above note can be solved using kubectl apply. "Apply" is the incrementable way, while
    "create" is imperative.
    - I tried to access it from Service Post with /swagger (bff) and got nothing... =/
    - kubectl logs pod_name to see what happened...internal port was 80.
    - Nice, lets describe port naming on service definition:
        - nodePort
            This setting makes the service visible outside the Kubernetes cluster by the node’s IP
            address and the port number declared in this property. The service also has to be of
            type NodePort (if this field isn’t specified, Kubernetes will allocate a node port automatically).
        - port
            Expose the service on the specified port internally within the cluster. That is, the
            service becomes visible on this port, and will send requests made to this port to the
            pods selected by the service.
        - targetPort
            This is the port on the pod that the request gets sent to. Your application needs to be
            listening for network requests on this port for the service to work.
    - Even after configuring the ports as following, I´m not getting response from container. Lets keep trying.
    - After some researching, NodePort wont work on AKS to expose a service, only loadbalance.

BFF is working. Of course the microservices arent deployed.
We have 7-9 projects inter-connected in order to make telepreco-poc work.
Use just plain k8s will be painful, because each repository will require a pod and service deployment each,
at minimum. Let´s study "Helm" to aggregate all deployments in just one config file.
    - Searching on the web, I realised that AKS is Helm Ready. I needed to install chocolatey in order to
    install Helm. I got Helm 3. It worked on our AKS cluster rightaway. So far so good.

Ok...suppose I got a helm chart with all microservices...how could they talk to each other.
Before I dive into Helm, I must ensure I can make pods talks throught services with plain k8s. Lets Go:
    - When we expose a service, we must program a way to discover the service ClusterIP dinamically inside
    each microservice (pod). How^?
    - If we create the service ClusterIP before the Pod, the Pod will get host ip and port for each
    registered services. I saw some examples on internet where when inside the pod using SSH, it curl
    the service using something like: curl http://$SERVICENAME_SERVICE_HOST:SERVICENAME_SERVICE_PORT.
    - Good, but how to access this values inside aspnetcore3 app?
    - Seems like just configuration.GetValue<string>("ExtraSettingNotInSettingsFile"); straight from:
    {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT
    Well, I need to ensure services are created first. In Yaml, the resources are created in the order
    they appear.
    - I created Sac microservice on k8s and re-created bff. Also need to make sure what are those enviroment
    variables, so let´s find a command to check pod env. vars.
    -> kubectl exec -it deploy-poc-telepreco-bff-asterisk-bf547db95-87xrb -- sh
    Once entered in pod's bash:
    -> printenv
    And found the SAC envs:
    SVC_POC_TELEPRECO_SAC_SERVICE_PORT
    SVC_POC_TELEPRECO_SAC_SERVICE_HOST
    - Lets put it on telepreco bff and see if it gets the host and IP.
    Modifies are on bff´s SacService, on ctor. Check it out. And lets test.
    - Worked!
    - I realised a problem here: In order to get the url to access a service, we
    need the service name, like SVC_POC_TELEPRECO_SAC. But what will happen if 
    the service name changes? Are we gonna change on code manually? The solution is
    Helm Templating. We are going to achieve this later.
    For now, let´s plug the other microservices... 

Just installed spekt8 (https://github.com/spekt8/spekt8)
To access: kubectl port-forward deployment/spekt8 3000:3000
then http://localhost:3000. Trust me, localhost.
This Tool is rubish...already deleted it.

Nice, I plugged in the other microservices using Helm.
Tested Helm Install, Upgrade, all working well.
Let´s upgrade our chart to a chart umbrela, with each microservice
a single chart, and each kubernetes object a yaml file.

Helm Umbrella is working! Much more organized than before. Nice.
Next step: plug a rabbitmq, mongodb and postgreSQl services.
To install rabbit, seems that I need to add a repository in order to
helm find the chart.
    helm repo add azure-marketplace https://marketplace.azurecr.io/helm/v1/repo
No...I was wrong...Seems like adding repositories to helm is for using "install"
commands. As I´m building my own chart, and I´m adding habbitmq as a dependency,
I need to add just the dependency entry on Charts.yaml.
I´m trying to apply an upgrade to the chart but is not working.
It´s saying: Error: found in Chart.yaml, but missing in charts/ directory: rabbitmq-ha.
Ha...It´s mandatory that I enter on the helm chart folder and type:
    helm dep build
    helm dep update (if you want to repeat later)
When I did this, it generated a tgz file inside charts folder. After this,
the habbitmq worked.
To access rabbitmq:
    kubectl port-forward poc-telepreco-rabbitmq-0 7000:15672

Cant Login into rabbitmq. We must set rabbitmq login and pass. But where? I can see
a lot of examples using helm install, but not with chart as dependency.
Solution: add a values.yaml with rabbitmq config (check file). I wrote
down the credentials and worked. Needed to reinstall to make changes.

Ahh...I would like to have pods with containers pulled when helm install,
but unfortunately they are not re-created, so I need to uninstall every time.
How to accomplish this? Dont know and its not important now.
So, lets install mongodb. Already added it as dependency.
Lets restart microservice-pessoa to check the name of the mongodbservice
    -> kubectl scale deployment deploy-poc-telepreco-pessoa --replicas=0
    -> kubectl scale deployment deploy-poc-telepreco-pessoa --replicas=1
    (just scale to zero, than to one again).

Mongodb Working. It required auth from me, but I disabled it just to connect,
on values.yaml again.
Since mongodb is working on pessoa, lets try rabbitmq. It´s working nice.
Lets install postgre...should be easy
Oh, stopping the deploy a little bi...lets install dashboard?
Dont run these commands: See why next
    -> kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml
    -> kubectl proxy
    -> http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.
Seems like this is not the right way to run dashboard, cos there is a AKS way to do this.
    -> az aks browse --resource-group Arquitetura --name k8s-arquitetura
    -> kubectl create clusterrolebinding kubernetes-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
Voilá! Kubernetes Dashboard.

I´m trying to move away from enviroment variables inside pods to the Core-Dns Solution. The coredns listens for services
and exposes them like myservice.mydomain.com inside the cluster. Much better solution.
Unfortunately, Helm will not help this. When I add the custom config, the helm complains that the object Already
exists in the cluster, what is completely right: aks deploys a core-dns object by default!
Unfortunately, to configure the core-dns we need to use kubectl apply. This is a flaw in Helm and it´s not perfect
as I was expecting, really a lame.
So, Just to ensure: to configure cluster dns, forget helm. Do it straight in k8s.
I lost 2 hours trying to make core-dns work. Gonna give up and keep using enviroment variables. Lets try again later.
After many days I came back here to say that core-dns was always working on AKS, I just need to use the hostnames...
The next plausible step is make sure all microservices are connected. On it.

I´m so lazy...and deploy this is very tiring.
Lets study some market products whom can help our setup. See:
    Istio (service mesh, hard to configure)
    Linkerd (service mesh too...but lightweight and promisses simplicity)
    Gloo (built on top of envoy)
    Kong
See more below about why we declined the use of service meshes.
    
I developed a easy and centralized (on data-contracts) way to get services host and IP.
You can check on ServiceDiscovery folder on MCarDataContracts and on gRPC services on bff.
Lets test this new way? On Pessoa first.

I just modified Bff to test pessoa. Built BFF and pushed to dockerhub. But, just a kubectl apply
will cause the image to be pulled? Seems not because the pod wasnt restarted. So...
The solution is: put imagePullPolicy: Always on Pod.yaml and image:latest. And because
this is a deployment, the pod will be recreated. Cool. Lets update all pods to this way.
PROBLEM: asterisk-microservice is not starting and causing incremental pod creation with 
"evicted" status. Command to delete these failed pods:
     kubectl -n default delete pods --field-selector=status.phase=Failed
Too many logs and many tries to connect to BFF. Lets delete it and connect them first.
After some time out of this adventure, lets continue>

We got a problem here: after a clear install, the rabbitmq lose its queues and exchanges.
So we need to register them every time, but by hand? Surely not. Programatically yes.
Kubernetes Jobs are pods that run a container and die, or run like a cron. We choose to run
a job and die, with a py scripty that registers those rabbit objects and also create some
postgres databases, that are not created automatically. This py script is under a dockerfile,
of course, that run a python-alpine dist. It runs the script and done.
The poc-telepreco top chart got only sub charts like an umbrella, but now it got a template.
Its our job. But how make it run after the postgres and rabbit services are available? How
to make sure? 

Helm Hooks are the answer. Check the chart annotations. We can tell helm when this yaml will
run and even define weights:
Helm Hook Weights are read by helm by ascendent count. So hook 1 will happen before hook 2 
and so on. Very nice to define a queue of kubernetes objects to be deployed.
For the attendance and budget microservices, that needs orcamento and atendimento databases
created before they deploy, its important to create these microservices after all others.
So they weight are the highest of all. Also I need to configure their docker to perform
migrate in order to create the tables structure. Lets get into it?

Changing subject a little: Service meshes is meant to work with microservice architecture
other than event driven. Or so called: microservice that calls microservices. On Our architecture,
services mesh will not work because of it event driven nature. We are researching more this field.

So, back to the frontend problem:
Problem: how to make a frontend app know the way to a kubernetes external service?
it is worth remembering that it is not worth setting the ip in hand inside the front
or picking up via var envs in the build time.
Solution: a kubernetes application that has more than one external service is an anti-pattern. We should have
just an external ip, served by an ingress scheme. nginx-ingress comes to the rescue. Stays like this:
    <external-ip>/service1
    <external-ip>/service2
    <external-ip>/service3
Each service above being a kubernetes service with a cluster-IP. The ingress is responsible for making the redirect.
This solves the problem of the front not knowing the service ip. The service ip is now a path of the same ip.
Nginx-ingress is a helm chart and all configuration must be done inside the helm.
Today I will implement this scheme. Due to its importance, it is a practice that will be used for all future k8 applications.
Notes about above:
    - Tutorial told me to create a new namespace to create my ingresses yaml. But how, since helm only manages apps inside
    just one namespace (its a limitation). No problem let it create the resources inside the namespace helm choses (default?)
    Good to notice, also, is that we can specify the namespace we want to install helm app. Add --namespace on the helm install
    command. If the namespace doesnt exist, it creates one. Good to create test and dev apps, sharing the same cluster.
    - If we map /xxx into a service, the application understand xxx/something as just /something. In the bff, where we
    use Swagger, the tool will try to connect to <ip>/swagger, but now ingress is <ip>/xxx/swagger. So it will not find the url and
    will crash. We need to config swagger to connect to his own api considering the new relative path, so our controllers too.
    - The frontend were mapped into /. But static files begins the url with `static`. We had to configure a new ingress object
    solely for these static files. Ingress doesnt understand what is not explicit mapped.
    - To map the service that deal with signalR (websockets), we got to set a row of new configs, and you can appreciate them
    on the BffAsterisk.yaml.  The configs are quite self-explanable.

Well. So good so far, my friend. Helm Project seems to be well functioning. We managed to achieve lift and ship.
There is one thing that we should achieve by all means necessary: tracing. Here why:
It's concept must be well defined, with sample tests, because tracing can't be implemented later after the
production stage is ready. We would need tracing abilities a lot to find and fix errors. Observability is one
core concept in microservices and without it we are in the dark.
So, continuing and bringing forth more infos: Unlike service meshes, where tracing is done in a layer above the
application (in this case the infrastructure topology, where proxies are put as a sidecarts to every machine,
to intercept calls and log them into the server to compose metrics), we are aiming a trace strategy solely generated
in application layer, that is, done inside our microservices code. We need this because we have a Event Based
architecture, where messages often goes as Integration Events on MQs. So, let me present the OpenTracing specification:

The OpenTracing specification: A tool agnostic way to tracing. Write using this standard and you can use any
tools OpenTracing compatible, being capable to change with no pain. This is very extensible and we can compare
to HTTP. We use HTTP for req/res and we can change from apache to nginx.
Traces in OpenTracing are defined by an atomic concept: a span. A trace is a graph of spans, not cyclic, where
the edges of the spans are called references.

Each Span encapsulates the following state:
    An operation name
    A start timestamp
    A finish timestamp
    A set of zero or more key:value Span Tags. The keys must be strings. The values may be strings, bools, or numeric types.
    A set of zero or more Span Logs, each of which is itself a key:value map paired with a timestamp.
        The keys must be strings, though the values may be of any type. Not all OpenTracing implementations must support every value type.
    A SpanContext
    References to zero or more causally-related Spans (via the SpanContext of those related Spans)

A span can reference zero or more other spancontexts. This relationship can be of ChildOf or FollowFrom.
    ChildOf: a span that depends on another. When I say depends, means that it expects response.
    For example: when a BFF calls Cliente using gRPC:
        Cliente side is ChildOf BFF side
    FollowFrom: a child span that doesnt depends in any way of parent span. This is interesting for our
    Fire and Forget kind of operation (MQs)
Keywords from Jaeger:
    Agent – A network daemon that listens for spans sent over User Datagram Protocol.
    Client – The component that implements the OpenTracing API for distributed tracing.
    Collector – The component that receives spans and adds them into a queue to be processed.
    Console – A UI that enables users to visualize their distributed tracing data.
    Query – A service that fetches traces from storage.
    Span – The logical unit of work in Jaeger, which includes the name, starting time and duration of the operation.
    Trace – The way Jaeger presents execution requests. A trace is composed of at least one span.
Just to record a possible Observability stack for our architecute:
A basic level of Observability:
    Elasticsearch-operator -> Tool to store and search for logs (and also maybe tracing)
    Jaeger-operator -> Tracing tool that uses OpenTracing standard
    Prometheus-operator -> systems and service monitoring system. It collects metrics from
        configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true.
    Weavescope -> Weave Scope automatically detects processes, containers, hosts.
    Fluent-bit -> Log Processor and Forwarder which allows you to collect data/logs from different sources
    Kibana -> Visualize Elasticsearch Data
    Grafana -> Monitoring Tool: its focus on data visualization.
So, how Observability would work here:
Following the three pilars of Observability, we got:
    Tracing: OpenTracing Jarger
    Logging: Fluent-Bit, that sends logs and metrics from microservices to
        Elasticsearch, which we can visualize using kibana and prometeus
    Metrics: Wavescope for infrastructure, Prometeus for Events
        and alerts and grafana for all purpose monitoring.