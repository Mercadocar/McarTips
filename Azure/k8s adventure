Kubernetes/Helm/Azure/DevOps Adventure.
A poor man guide to master orchestration.

arquitetura@mercadocarmercantil.onmicrosoft.com
Dovu4129

I just created the k8s on azure. Let´s try to access it locally.
    Gotta install Azure CLI.
    https://docs.microsoft.com/pt-br/cli/azure/install-azure-cli?view=azure-cli-latest
    Run az login
    Set the same cred. used to gotto azure portal.
    "az aks install-cli" to install AKS. Set the variables as it commands.
    Then find the resource group, the k8s cluster name and run:
    (this command below actually works today)
    az aks get-credentials --resource-group Arquitetura --name k8s-arquitetura
    "kubectl get nodes" to see if works
    Nice. Now we can control k8s on azure locally.
Let´s try to deploy our Telepreço POC on k8s.
Then I realised that our docker images are hosted on dockerhub (private ones)
We need to let kubernetes know it and make it able to pull images from there.
So lets go:
    export DOCKER_REGISTRY_SERVER=https://index.docker.io/v1/
    export DOCKER_USER=Type your dockerhub username, same as when you `docker login`
    export DOCKER_EMAIL=Type your dockerhub email, same as when you `docker login`
    export DOCKER_PASSWORD=Type your dockerhub pw, same as when you `docker login`

    kubectl create secret docker-registry myregistrykey \
    --docker-server=$DOCKER_REGISTRY_SERVER \
    --docker-username=$DOCKER_USER \
    --docker-password=$DOCKER_PASSWORD \
    --docker-email=$DOCKER_EMAIL

    Then the image will be like this on yaml:
    image: index.docker.io/rodoflho/mcarbabybfftelepreco:latest
These commands were for private repos on docker hub, but when they are public
just ignore and lets continue.
Now, lets edit the yaml to make BFF work. Gonna list my struggle in order of happening:
    - API versions: v1, v1-beta, I dont know...some of them are not contemplating
    some keywords like deployment (in vase of v1) and NodePort.
    - Type: NodePort should be in same level than spec
    - First I added an deployment, ok. But then tried to add a service too, on the same file.
    The service wanted to be created, but deployment not. I would like something incrementable.
    - Nice! The above note can be solved using kubectl apply. "Apply" is the incrementable way, while
    "create" is imperative.
    - I tried to access it from Service Post with /swagger (bff) and got nothing... =/
    - kubectl logs pod_name to see what happened...internal port was 80.
    - Nice, lets describe port naming on service definition:
        - nodePort
            This setting makes the service visible outside the Kubernetes cluster by the node’s IP
            address and the port number declared in this property. The service also has to be of
            type NodePort (if this field isn’t specified, Kubernetes will allocate a node port automatically).
        - port
            Expose the service on the specified port internally within the cluster. That is, the
            service becomes visible on this port, and will send requests made to this port to the
            pods selected by the service.
        - targetPort
            This is the port on the pod that the request gets sent to. Your application needs to be
            listening for network requests on this port for the service to work.
    - Even after configuring the ports as following, I´m not getting response from container. Lets keep trying.
    - After some researching, NodePort wont work on AKS to expose a service, only loadbalance.

BFF is working. Of course the microservices arent deployed.
We have 7-9 projects inter-connected in order to make telepreco-poc work.
Use just plain k8s will be painful, because each repository will require a pod and service deployment each,
at minimum. Let´s study "Helm" to aggregate all deployments in just one config file.
    - Searching on the web, I realised that AKS is Helm Ready. I needed to install chocolatey in order to
    install Helm. I got Helm 3. It worked on our AKS cluster rightaway. So far so good.
Ok...suppose I got a helm chart with all microservices...how could they talk to each other.
Before I dive into Helm, I must ensure I can make pods talks thought services with plain k8s. Lets Go:
    - When we expose a service, we must program a way to discover the service ClusterIP dinamically inside
    each microservice (pod). How^?
    - If we create the service ClusterIP before the Pod, the Pod will get host ip and port for each
    registered services. I saw some examples on internet where when inside the pod using SSH, it curl
    the service using something like: curl http://$SERVICENAME_SERVICE_HOST:SERVICENAME_SERVICE_PORT.
    Good, but how to access this values inside aspnetcore3 app?
    Seems like just configuration.GetValue<string>("ExtraSettingNotInSettingsFile"); straight from:
    {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT
    Well, I need to ensure services are created first. In Yaml, the resources are created in the order
    they appear.
    I created Sac microservice on k8s and re-created bff. Also need to make sure what are those enviroment
    variables, so let´s find a command to check pod env. vars.
    -> kubectl exec -it deploy-poc-telepreco-bff-867765bf6-xx6kr -- sh
    Once entered in pod's bash:
    -> printenv
    And found the SAC envs:
    SVC_POC_TELEPRECO_SAC_SERVICE_PORT
    SVC_POC_TELEPRECO_SAC_SERVICE_HOST
    Lets put it on telepreco bff and see if it gets the host and IP.
    Modifies are on bff´s SacService, on ctor. Check it out. And lets test.
    Worked!
    I realised a problem here: In order to get the url to access a service, we
    need the service name, like SVC_POC_TELEPRECO_SAC. But what will happen if 
    the service name changes? Are we gonna change on code manually? The solution is
    Helm Templating. We are going to achieve this later.
    For now, let´s plug the other microservices... 
Just installed spekt8 (https://github.com/spekt8/spekt8)
To access: kubectl port-forward deployment/spekt8 3000:3000
then http://localhost:3000. Trust me, localhost.
Nice, I plugged in the other microservices using Helm.
Tested Helm Install, Upgrade, all working well.
Let´s upgrade our chart to a chart umbrela, with each microservice
a single chart, and each kubernetes object a yaml file.
Helm Umbrella is working! Much more organized than before. Nice.
Next step: plug a rabbitmq, mongodb and postgreSQl services.
To install rabbit, seems that I need to add a repository in order to
helm find the chart.
    helm repo add azure-marketplace https://marketplace.azurecr.io/helm/v1/repo
No...I was wrong...Seems like adding repositories to helm is for using "install"
commands. As I´m building my own chart, and I´m adding habbitmq as a dependency,
I need to add just the dependency entry on Charts.yaml.
I´m trying to apply an upgrade to the chart but is not working.
It´s saying: Error: found in Chart.yaml, but missing in charts/ directory: rabbitmq-ha.
Ha...It´s mandatory that I enter on the helm chart folder and type:
    helm dep build
When I did this, it generated a tgz file inside charts folder. After this,
the habbitmq worked.
To access rabbitmq:
    kubectl port-forward poc-telepreco-rabbitmq-ha-0 7000:15672
Cant Login. We must set rabbitmq login and pass. But where? I can see
a lot of examples using helm install, but not with chart as dependency.