Kubernetes/Helm/Azure/DevOps Adventure.
A poor man guide to master orchestration.

arquitetura@mercadocarmercantil.onmicrosoft.com
Dovu41293

I just created the k8s on azure. Let´s try to access it locally.
    Gotta install Azure CLI.
    https://docs.microsoft.com/pt-br/cli/azure/install-azure-cli?view=azure-cli-latest
    Run az login
    Set the same cred. used to gotto azure portal.
    "az aks install-cli" to install AKS. Set the variables as it commands.
    Then find the resource group, the k8s cluster name and run:
    (this command below actually works today)
    az aks get-credentials --resource-group Arquitetura --name k8s-arquitetura
    "kubectl get nodes" to see if works
    Nice. Now we can control k8s on azure locally.
Let´s try to deploy our Telepreço POC on k8s.
Then I realised that our docker images are hosted on dockerhub (private ones)
We need to let kubernetes know it and make it able to pull images from there.
So lets go:
    export DOCKER_REGISTRY_SERVER=https://index.docker.io/v1/
    export DOCKER_USER=Type your dockerhub username, same as when you `docker login`
    export DOCKER_EMAIL=Type your dockerhub email, same as when you `docker login`
    export DOCKER_PASSWORD=Type your dockerhub pw, same as when you `docker login`

    kubectl create secret docker-registry myregistrykey \
    --docker-server=$DOCKER_REGISTRY_SERVER \
    --docker-username=$DOCKER_USER \
    --docker-password=$DOCKER_PASSWORD \
    --docker-email=$DOCKER_EMAIL

    Then the image will be like this on yaml:
    image: index.docker.io/rodoflho/mcarbabybfftelepreco:latest
These commands were for private repos on docker hub, but when they are public
just ignore and lets continue.
Now, lets edit the yaml to make BFF work. Gonna list my struggle in order of happening:
    - API versions: v1, v1-beta, I dont know...some of them are not contemplating
    some keywords like deployment (in vase of v1) and NodePort.
    - Type: NodePort should be in same level than spec
    - First I added an deployment, ok. But then tried to add a service too, on the same file.
    The service wanted to be created, but deployment not. I would like something incrementable.
    - Nice! The above note can be solved using kubectl apply. "Apply" is the incrementable way, while
    "create" is imperative.
    - I tried to access it from Service Post with /swagger (bff) and got nothing... =/
    - kubectl logs pod_name to see what happened...internal port was 80.
    - Nice, lets describe port naming on service definition:
        - nodePort
            This setting makes the service visible outside the Kubernetes cluster by the node’s IP
            address and the port number declared in this property. The service also has to be of
            type NodePort (if this field isn’t specified, Kubernetes will allocate a node port automatically).
        - port
            Expose the service on the specified port internally within the cluster. That is, the
            service becomes visible on this port, and will send requests made to this port to the
            pods selected by the service.
        - targetPort
            This is the port on the pod that the request gets sent to. Your application needs to be
            listening for network requests on this port for the service to work.
    - Even after configuring the ports as following, I´m not getting response from container. Lets keep trying.
    - After some researching, NodePort wont work on AKS to expose a service, only loadbalance.

BFF is working. Of course the microservices arent deployed.
We have 7-9 projects inter-connected in order to make telepreco-poc work.
Use just plain k8s will be painful, because each repository will require a pod and service deployment each,
at minimum. Let´s study "Helm" to aggregate all deployments in just one config file.
    - Searching on the web, I realised that AKS is Helm Ready. I needed to install chocolatey in order to
    install Helm. I got Helm 3. It worked on our AKS cluster rightaway. So far so good.
Ok...suppose I got a helm chart with all microservices...how could they talk to each other.
Before I dive into Helm, I must ensure I can make pods talks thought services with plain k8s. Lets Go:
    - When we expose a service, we must program a way to discover the service ClusterIP dinamically inside
    each microservice (pod). How^?
    - If we create the service ClusterIP before the Pod, the Pod will get host ip and port for each
    registered services. I saw some examples on internet where when inside the pod using SSH, it curl
    the service using something like: curl http://$SERVICENAME_SERVICE_HOST:SERVICENAME_SERVICE_PORT.
    Good, but how to access this values inside aspnetcore3 app?
    Seems like just configuration.GetValue<string>("ExtraSettingNotInSettingsFile"); straight from:
    {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT
    Well, I need to ensure services are created first. In Yaml, the resources are created in the order
    they appear.
    I created Sac microservice on k8s and re-created bff. Also need to make sure what are those enviroment
    variables, so let´s find a command to check pod env. vars.
    -> kubectl exec -it deploy-poc-telepreco-bff-867765bf6-xx6kr -- sh
    Once entered in pod's bash:
    -> printenv
    And found the SAC envs:
    SVC_POC_TELEPRECO_SAC_SERVICE_PORT
    SVC_POC_TELEPRECO_SAC_SERVICE_HOST
    Lets put it on telepreco bff and see if it gets the host and IP.
    Modifies are on bff´s SacService, on ctor. Check it out. And lets test.
    Worked!
    I realised a problem here: In order to get the url to access a service, we
    need the service name, like SVC_POC_TELEPRECO_SAC. But what will happen if 
    the service name changes? Are we gonna change on code manually? The solution is
    Helm Templating. We are going to achieve this later.
    For now, let´s plug the other microservices... 
Just installed spekt8 (https://github.com/spekt8/spekt8)
To access: kubectl port-forward deployment/spekt8 3000:3000
then http://localhost:3000. Trust me, localhost.
Nice, I plugged in the other microservices using Helm.
Tested Helm Install, Upgrade, all working well.
Let´s upgrade our chart to a chart umbrela, with each microservice
a single chart, and each kubernetes object a yaml file.
Helm Umbrella is working! Much more organized than before. Nice.
Next step: plug a rabbitmq, mongodb and postgreSQl services.
To install rabbit, seems that I need to add a repository in order to
helm find the chart.
    helm repo add azure-marketplace https://marketplace.azurecr.io/helm/v1/repo
No...I was wrong...Seems like adding repositories to helm is for using "install"
commands. As I´m building my own chart, and I´m adding habbitmq as a dependency,
I need to add just the dependency entry on Charts.yaml.
I´m trying to apply an upgrade to the chart but is not working.
It´s saying: Error: found in Chart.yaml, but missing in charts/ directory: rabbitmq-ha.
Ha...It´s mandatory that I enter on the helm chart folder and type:
    helm dep build
    helm dep update (if you want to repeat later)
When I did this, it generated a tgz file inside charts folder. After this,
the habbitmq worked.
To access rabbitmq:
    kubectl port-forward poc-telepreco-rabbitmq-0 7000:15672
Cant Login. We must set rabbitmq login and pass. But where? I can see
a lot of examples using helm install, but not with chart as dependency.
Solution: add a values.yaml with rabbitmq config (check file). I wrote
down the credentials and worked. Needed to reinstall to make changes.
Ahh...I would like to have pods with containers pulled when helm install,
but unfortunately they are not re-created, so I need to uninstall every time.
How to accomplish this? Dont know and its not important now.
So, lets install mongodb. Already added it as dependency.
Lets restart microservice-pessoa to check the name of the mongodbservice
    -> kubectl scale deployment deploy-poc-telepreco-pessoa --replicas=0
    -> kubectl scale deployment deploy-poc-telepreco-pessoa --replicas=1
    (just scale to zero, than to one again).
Mongodb Working. It required auth from me, but I disabled it just to connect,
on values.yaml again.
Since mongodb is working on pessoa, lets try rabbitmq. It´s working nice.
Lets install postgre...should be easy
Oh, stopping the deploy a little bi...lets install dashboard?
Dont run these commands: See why next
    -> kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml
    -> kubectl proxy
    -> http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.
Seems like this is not the right way to run dashboard, cos there is a AKS way to do this.
    -> az aks browse --resource-group Arquitetura --name k8s-arquitetura
    -> kubectl create clusterrolebinding kubernetes-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
Voilá! Kubernetes Dashboard.
I´m trying to move away from enviroment variables inside pods to the Core-Dns Solution. The coredns listens for services
and exposes them like myservice.mydomain.com inside the cluster. Much better solution.
Unfortunately, Helm will not help this. When I add the custom config, the helm complains that the object Already
exists in the cluster, what is completely right: aks deploys a core-dns object by default. 
Unfortunately, to configure the core-dns we need to use kubectl apply. This is a flaw in Helm and it´s not perfect
as I was expecting, really a lame.
So, Just to ensure: to configure cluster dns, forget helm. Do it straight in k8s.
I lost 2 hours trying to make core-dns work. Gonna give up and keep using enviroment variables. Lets try again later.
The next plausible step is make sure all microservices are connected. On it.
I´m so lazy...and deploy this is very tiring.
Lets study some market products whom can help our setup. See:
    Istio (service mesh, hard to configure)
    Linkerd (service mesh too...but lightweight and promisses simplicity)
    Gloo (built on top of envoy)
    Kong
I developed a easy and centralized (on data-contracts) way to get services host and IP.
You can check on ServiceDiscovery folder on MCarDataContracts and on gRPC services on bff.
Lets test this new way? On Pessoa first.
I just modified Bff to test pessoa. Built BFF and pushed to dockerhub. But, just a kubectl apply
will cause the image to be pulled? Seems not because the pod wasnt restarted. So...
The solution is: put imagePullPolicy: Always on Pod.yaml and image:latest. And because
this is a deployment, the pod will be recreated. Cool. Lets update all pods to this way.
PROBLEM: asterisk-microservice is not starting and causing incremental pod creation with 
"evicted" status. Command to delete these failed pods:
     kubectl -n default delete pods --field-selector=status.phase=Failed
Too many logs and many tries to connect to BFF. Lets delete it and connect them first.
After some time out of this adventure, lets continue>
We got a problem here: after a clear install, the rabbitmq lose its queues and exchanges.
So we need to register them every time, but by hand? Surely not. Programatically yes.
Kubernetes Jobs are pods that run a container and die, or run like a cron. We choose to run
a job and die, with a py scripty that registers those rabbit objects and also create some
postgres databases, that are not created automatically. This py script is under a dockerfile,
of course, that run a python-alpine dist. It runs the script and done.
The poc-telepreco top chart got only sub charts like an umbrella, but now it got a template.
Its our job. But how make it run after the postgres and rabbit services are available? How
to make sure? 
Helm Hooks are the answer. Check the chart annotations. We can tell helm when this yaml will
run and even define weights:
Helm Hook Weights are read by helm by ascendent count. So hook 1 will happen before hook 2 
and so on. Very nice to define a queue of kubernetes objects to be deployed.
For the attendance and budget microservices, that needs orcamento and atendimento databases
created before they deploy, its important to create these microservices after all others.
So they weight are the highest of all. Also I need to configure their docker to perform
migrate in order to create the tables structure. Lets get into it?

 
